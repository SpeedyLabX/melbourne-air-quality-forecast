{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melbourne Social Media Data Collection for Air Quality Analysis\n",
    "# Research Notebook for Environmental and Social Sensing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook is a critical component of the \"Proactive Air Quality Forecasting and Health Warning System in Melbourne\" project. It focuses on collecting data from various social media platforms (Twitter, Reddit, Facebook) to derive a **User Observations Index (UOI)**. This index captures real-time, community-reported observations of smoke and air pollution, providing a valuable data source to complement official sensor data.\n",
    "\n",
    "### Key Features:\n",
    "- **Multi-Platform Collection**: Gathers data from Twitter, Reddit, and provides a framework for Facebook.\n",
    "- **Secure API Key Management**: Utilizes a `.env` file for secure credential storage.\n",
    "- **Robust & Resilient**: Comprehensive error handling and logging for each platform.\n",
    "- **Targeted Search**: Uses specific keywords and locations relevant to Melbourne's air quality.\n",
    "- **Structured Output**: Saves raw data in clean JSON format, ready for preprocessing and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 01:36:58,866 - INFO - üì± Melbourne Social Media Data Collection System\n",
      "2025-06-21 01:36:58,868 - INFO - ==================================================\n",
      "2025-06-21 01:36:58,869 - INFO - üî¨ Research Environment Initialized\n",
      "2025-06-21 01:36:58,870 - INFO - üìã Dependencies loaded and logger configured.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: SETUP, IMPORTS, AND LOGGER CONFIGURATION\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Melbourne Social Media Data Collection System for UOI\n",
    "=====================================================\n",
    "\n",
    "This notebook collects social media posts related to air quality in Melbourne\n",
    "from Twitter, Reddit, and Facebook.\n",
    "\n",
    "Author: Research Team (L√™ Nguy·ªÖn Gia H∆∞ng, Ho√†ng Ph·∫°m Gia B·∫£o, V√µ T·∫•n Ph√°t)\n",
    "Date: 2025\n",
    "Purpose: Collect data for User Observations Index (UOI) calculation.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "import pandas as pd\n",
    "import praw\n",
    "import tweepy\n",
    "import facebook\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- 1. UTF-8 Aware Logger Setup (Consistent with Project Standards) ---\n",
    "def setup_logger(log_file='logs/03_social_media_data_collection.log', level=logging.INFO):\n",
    "    \"\"\"Configures a logger to be UTF-8 aware for both console and file output.\"\"\"\n",
    "    # Create logs directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(level)\n",
    "    \n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "    \n",
    "    file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# --- 2. Initialize Environment ---\n",
    "load_dotenv()\n",
    "logger = setup_logger()\n",
    "\n",
    "logger.info(\"üì± Melbourne Social Media Data Collection System\")\n",
    "logger.info(\"=\" * 50)\n",
    "logger.info(\"üî¨ Research Environment Initialized\")\n",
    "logger.info(\"üìã Dependencies loaded and logger configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Research Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 01:36:58,896 - INFO - üîë Twitter credentials configured: ‚úÖ\n",
      "2025-06-21 01:36:58,897 - INFO - üîë Reddit credentials configured: ‚úÖ\n",
      "2025-06-21 01:36:58,898 - INFO - üîë Facebook credentials configured: ‚úÖ\n",
      "2025-06-21 01:36:58,899 - INFO - üéØ Research Configuration Loaded\n",
      "2025-06-21 01:36:58,899 - INFO - üíæ Output directory: ../../data/raw/social_media/\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: RESEARCH CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "class SocialMediaConfig:\n",
    "    \"\"\"Configuration class for social media data collection.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # --- API Credentials ---\n",
    "        self.TWITTER_BEARER_TOKEN = os.getenv('TWITTER_BEARER_TOKEN')\n",
    "        self.REDDIT_CLIENT_ID = os.getenv('REDDIT_CLIENT_ID')\n",
    "        self.REDDIT_CLIENT_SECRET = os.getenv('REDDIT_CLIENT_SECRET')\n",
    "        self.REDDIT_USER_AGENT = os.getenv('REDDIT_USER_AGENT')\n",
    "        self.FACEBOOK_ACCESS_TOKEN = os.getenv('FACEBOOK_ACCESS_TOKEN')\n",
    "\n",
    "        # --- Search Parameters ---\n",
    "        # Shared keywords for all platforms\n",
    "        self.KEYWORDS = [\"smoke\", \"air quality\", \"haze\", \"bushfire smoke\", \"air pollution\", \"PM2.5\"]\n",
    "        self.TWITTER_QUERY = f'(\"melbourne\" OR \"#melbweather\") ({\" OR \".join(self.KEYWORDS)}) -is:retweet'\n",
    "        self.REDDIT_SUBREDDITS = ['melbourne', 'australia']\n",
    "        self.FACEBOOK_GROUP_IDS = ['MelbourneWeatherWatch'] # Example, use actual ID\n",
    "        \n",
    "        # --- Data Collection Parameters ---\n",
    "        self.SEARCH_LIMIT_PER_PLATFORM = 500  # Max items to fetch per run\n",
    "        self.REQUEST_TIMEOUT = 30 # seconds\n",
    "        \n",
    "        # --- Output Configuration ---\n",
    "        self.OUTPUT_DIR = \"../../data/raw/social_media/\"\n",
    "        os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
    "        self.TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        self.OUTPUT_PATHS = {\n",
    "            \"twitter\": os.path.join(self.OUTPUT_DIR, f\"twitter_data_{self.TIMESTAMP}.json\"),\n",
    "            \"reddit\": os.path.join(self.OUTPUT_DIR, f\"reddit_data_{self.TIMESTAMP}.json\"),\n",
    "            \"facebook\": os.path.join(self.OUTPUT_DIR, f\"facebook_data_{self.TIMESTAMP}.json\")\n",
    "        }\n",
    "        \n",
    "    def validate_credentials(self):\n",
    "        \"\"\"Checks if essential API credentials are set.\"\"\"\n",
    "        if not all([self.TWITTER_BEARER_TOKEN]):\n",
    "            logger.warning(\"‚ö†Ô∏è Twitter API Bearer Token not found. Twitter collection will be skipped.\")\n",
    "        else: logger.info(\"üîë Twitter credentials configured: ‚úÖ\")\n",
    "            \n",
    "        if not all([self.REDDIT_CLIENT_ID, self.REDDIT_CLIENT_SECRET, self.REDDIT_USER_AGENT]):\n",
    "            logger.warning(\"‚ö†Ô∏è Reddit API credentials not found. Reddit collection will be skipped.\")\n",
    "        else: logger.info(\"üîë Reddit credentials configured: ‚úÖ\")\n",
    "            \n",
    "        if not self.FACEBOOK_ACCESS_TOKEN:\n",
    "            logger.warning(\"‚ö†Ô∏è Facebook Access Token not found. Facebook collection will be skipped.\")\n",
    "        else: logger.info(\"üîë Facebook credentials configured: ‚úÖ\")\n",
    "\n",
    "# --- Initialize configuration ---\n",
    "try:\n",
    "    config = SocialMediaConfig()\n",
    "    config.validate_credentials()\n",
    "    logger.info(f\"üéØ Research Configuration Loaded\")\n",
    "    logger.info(f\"üíæ Output directory: {config.OUTPUT_DIR}\")\n",
    "except Exception as e:\n",
    "    logger.critical(f\"‚ùå Could not initialize configuration: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Collection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 01:36:58,936 - INFO - API clients initialized successfully.\n",
      "2025-06-21 01:36:58,938 - INFO - üîß Social media collector instance created.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: CORE DATA COLLECTION CLASS (REVISED)\n",
    "# =============================================================================\n",
    "# This revised cell incorporates feedback to improve robustness.\n",
    "# Key Changes in `fetch_from_twitter`:\n",
    "#   - Uses `tweepy.Paginator` to correctly handle pagination and fetch up to the desired limit.\n",
    "#   - Adds specific error handling for `tweepy.errors.TooManyRequestsError`.\n",
    "#   - Provides a more accurate `tqdm` progress bar for the Twitter collection process.\n",
    "# =============================================================================\n",
    "\n",
    "class SocialMediaCollector:\n",
    "    \"\"\"Handles the logic for fetching data from social media APIs.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: SocialMediaConfig):\n",
    "        self.config = config\n",
    "        self.api_clients = {}\n",
    "        self._initialize_clients()\n",
    "        \n",
    "    def _initialize_clients(self):\n",
    "        \"\"\"Initializes API clients based on available credentials.\"\"\"\n",
    "        try:\n",
    "            if self.config.TWITTER_BEARER_TOKEN:\n",
    "                # Add wait_on_rate_limit to help with minor limit breaches\n",
    "                self.api_clients['twitter'] = tweepy.Client(\n",
    "                    self.config.TWITTER_BEARER_TOKEN,\n",
    "                    wait_on_rate_limit=True\n",
    "                )\n",
    "            if all([self.config.REDDIT_CLIENT_ID, self.config.REDDIT_CLIENT_SECRET, self.config.REDDIT_USER_AGENT]):\n",
    "                self.api_clients['reddit'] = praw.Reddit(\n",
    "                    client_id=self.config.REDDIT_CLIENT_ID,\n",
    "                    client_secret=self.config.REDDIT_CLIENT_SECRET,\n",
    "                    user_agent=self.config.REDDIT_USER_AGENT\n",
    "                )\n",
    "            if self.config.FACEBOOK_ACCESS_TOKEN:\n",
    "                self.api_clients['facebook'] = facebook.GraphAPI(self.config.FACEBOOK_ACCESS_TOKEN)\n",
    "            logger.info(\"API clients initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to initialize an API client: {e}\")\n",
    "\n",
    "    def _save_to_json(self, data: List[Dict], platform: str):\n",
    "        \"\"\"Saves collected data to a JSON file.\"\"\"\n",
    "        if not data:\n",
    "            logger.info(f\"No data to save for {platform}.\")\n",
    "            return\n",
    "        \n",
    "        path = self.config.OUTPUT_PATHS[platform]\n",
    "        try:\n",
    "            with open(path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "            logger.info(f\"üíæ Successfully saved {len(data)} records for {platform} to {path}\")\n",
    "        except IOError as e:\n",
    "            logger.error(f\"‚ùå Could not write to file {path}: {e}\")\n",
    "\n",
    "    def fetch_from_twitter(self) -> List[Dict]:\n",
    "        \"\"\"Fetches tweets using the Twitter API v2, with pagination and rate limit handling.\"\"\"\n",
    "        if 'twitter' not in self.api_clients:\n",
    "            logger.warning(\"Skipping Twitter: client not initialized.\")\n",
    "            return []\n",
    "            \n",
    "        client = self.api_clients['twitter']\n",
    "        collected_tweets = []\n",
    "        logger.info(f\"üê¶ Fetching up to {self.config.SEARCH_LIMIT_PER_PLATFORM} tweets with query: {self.config.TWITTER_QUERY}\")\n",
    "        \n",
    "        try:\n",
    "            # Use Paginator to automatically handle fetching multiple pages of results\n",
    "            paginator = tweepy.Paginator(\n",
    "                client.search_recent_tweets,\n",
    "                query=self.config.TWITTER_QUERY,\n",
    "                tweet_fields=[\"created_at\", \"author_id\", \"public_metrics\", \"geo\"],\n",
    "                max_results=100  # Max per page, Paginator handles the rest\n",
    "            ).flatten(limit=self.config.SEARCH_LIMIT_PER_PLATFORM)\n",
    "\n",
    "            # Loop through the paginated results with a progress bar\n",
    "            for tweet in tqdm(paginator, desc=\"Processing Tweets\", unit=\"tweet\", total=self.config.SEARCH_LIMIT_PER_PLATFORM):\n",
    "                collected_tweets.append({\n",
    "                    'platform': 'twitter',\n",
    "                    'id': tweet.id,\n",
    "                    'text': tweet.text,\n",
    "                    'created_at': tweet.created_at.isoformat(),\n",
    "                    'author_id': tweet.author_id\n",
    "                })\n",
    "            \n",
    "            if not collected_tweets:\n",
    "                logger.info(\"No tweets found for the given query.\")\n",
    "\n",
    "        except tweepy.errors.TooManyRequestsError:\n",
    "            logger.error(\"‚ùå Twitter API rate limit exceeded. Collection for Twitter has stopped. Please wait and try again later.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå An unexpected error occurred during Twitter fetch: {e}\")\n",
    "        \n",
    "        self._save_to_json(collected_tweets, 'twitter')\n",
    "        return collected_tweets\n",
    "\n",
    "    def fetch_from_reddit(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Fetches posts from Reddit with improved error handling.\n",
    "        This version isolates failures to a single subreddit, allowing the\n",
    "        collection to continue with other subreddits if one fails.\n",
    "        \"\"\"\n",
    "        if 'reddit' not in self.api_clients:\n",
    "            logger.warning(\"Skipping Reddit: client not initialized.\")\n",
    "            return []\n",
    "            \n",
    "        reddit = self.api_clients['reddit']\n",
    "        collected_posts = []\n",
    "        query = ' OR '.join(self.config.KEYWORDS)\n",
    "        logger.info(f\"ü§ñ Fetching from Reddit subreddits: {self.config.REDDIT_SUBREDDITS}\")\n",
    "        \n",
    "        # Calculate an approximate limit per subreddit\n",
    "        limit_per_sub = self.config.SEARCH_LIMIT_PER_PLATFORM // len(self.config.REDDIT_SUBREDDITS)\n",
    "        if limit_per_sub == 0: limit_per_sub = 1 # Ensure at least 1 post is fetched if limit is small\n",
    "        \n",
    "        for subreddit_name in self.config.REDDIT_SUBREDDITS:\n",
    "            try:\n",
    "                logger.info(f\"-> Searching r/{subreddit_name}...\")\n",
    "                subreddit = reddit.subreddit(subreddit_name)\n",
    "                search_results = subreddit.search(query, sort='new', limit=limit_per_sub)\n",
    "                \n",
    "                # Use a list to buffer results from the generator, making it more resilient to mid-stream errors\n",
    "                subreddit_posts = list(tqdm(search_results, desc=f\"r/{subreddit_name}\", unit=\"post\"))\n",
    "                \n",
    "                for submission in subreddit_posts:\n",
    "                    collected_posts.append({\n",
    "                        'platform': 'reddit',\n",
    "                        'id': submission.id,\n",
    "                        'title': submission.title,\n",
    "                        'text': submission.selftext,\n",
    "                        'created_utc': datetime.fromtimestamp(submission.created_utc, tz=timezone.utc).isoformat(),\n",
    "                        'subreddit': subreddit_name,\n",
    "                        'score': submission.score,\n",
    "                        'url': submission.url\n",
    "                    })\n",
    "                logger.info(f\"-> Found {len(subreddit_posts)} posts in r/{subreddit_name}.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # By placing the try/except block here, if one subreddit fails,\n",
    "                # we log the error and the loop continues to the next one.\n",
    "                logger.error(f\"‚ùå An error occurred while fetching from r/{subreddit_name}: {e}\")\n",
    "                logger.warning(f\"--> Skipping to the next subreddit.\")\n",
    "                continue # Move to the next subreddit\n",
    "                \n",
    "        self._save_to_json(collected_posts, 'reddit')\n",
    "        return collected_posts\n",
    "    \n",
    "    def fetch_from_facebook(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Placeholder for fetching data from Facebook Groups/Pages.\n",
    "        NOTE: Requires a valid User or Page Access Token with appropriate permissions.\n",
    "        Access to public group data is highly restricted. This is a basic framework.\n",
    "        \"\"\"\n",
    "        if 'facebook' not in self.api_clients:\n",
    "            logger.warning(\"Skipping Facebook: client not initialized.\")\n",
    "            return []\n",
    "            \n",
    "        logger.warning(\"üîµ Facebook collection is a placeholder. API access is restricted and this function will not collect data.\")\n",
    "        # The following code is a template and will likely fail without a reviewed app and proper permissions.\n",
    "        # graph = self.api_clients['facebook']\n",
    "        # collected_posts = []\n",
    "        # try:\n",
    "        #     for group_id in self.config.FACEBOOK_GROUP_IDS:\n",
    "        #         feed = graph.get_connections(id=group_id, connection_name='feed',\n",
    "        #                                    fields='message,created_time,id,permalink_url')\n",
    "        #         # Process feed data here...\n",
    "        #     self._save_to_json(collected_posts, 'facebook')\n",
    "        # except facebook.GraphAPIError as e:\n",
    "        #     logger.error(f\"‚ùå Facebook Graph API Error: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Initialize the collector ---\\\n",
    "try:\n",
    "    collector = SocialMediaCollector(config)\n",
    "    logger.info(\"üîß Social media collector instance created.\")\n",
    "except Exception as e:\n",
    "    logger.critical(f\"‚ùå Failed to instantiate SocialMediaCollector: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Execute Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 01:36:58,980 - INFO - üöÄ Starting social media data collection pipeline...\n",
      "2025-06-21 01:36:58,980 - INFO - üê¶ Fetching up to 500 tweets with query: (\"melbourne\" OR \"#melbweather\") (smoke OR air quality OR haze OR bushfire smoke OR air pollution OR PM2.5) -is:retweet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e6d7386f2044e89097b482b3a44b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Tweets:   0%|          | 0/500 [00:00<?, ?tweet/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 01:36:59,464 - INFO - üíæ Successfully saved 12 records for twitter to ../../data/raw/social_media/twitter_data_20250621_013658.json\n",
      "2025-06-21 01:36:59,466 - INFO - ü§ñ Fetching from Reddit subreddits: ['melbourne', 'australia']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2a65e6e9684dd08df7b9ce86d67868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "r/melbourne: 0post [00:00, ?post/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 01:36:59,519 - WARNING - Retrying due to ConnectionError(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))) status: GET https://oauth.reddit.com/r/melbourne/search/\n",
      "2025-06-21 01:37:01,187 - WARNING - Retrying due to ConnectionError(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))) status: GET https://oauth.reddit.com/r/melbourne/search/\n",
      "2025-06-21 01:37:03,380 - ERROR - ‚ùå An error occurred during Reddit fetch: error with request ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "2025-06-21 01:37:03,381 - WARNING - üîµ Facebook collection is a placeholder. API access is restricted and this function will not collect data.\n",
      "2025-06-21 01:37:03,382 - INFO - ============================================================\n",
      "2025-06-21 01:37:03,385 - INFO - ‚úÖ SOCIAL MEDIA DATA COLLECTION COMPLETED!\n",
      "2025-06-21 01:37:03,386 - INFO - ‚è±Ô∏è  Total duration: 0:00:04.402947\n",
      "2025-06-21 01:37:03,387 - INFO - üìä Records collected:\n",
      "2025-06-21 01:37:03,388 - INFO -    - Twitter: 12 records saved to ../../data/raw/social_media/twitter_data_20250621_013658.json\n",
      "2025-06-21 01:37:03,390 - INFO -    - Reddit: 0 records saved to ../../data/raw/social_media/reddit_data_20250621_013658.json\n",
      "2025-06-21 01:37:03,391 - INFO -    - Facebook: 0 records (placeholder)\n",
      "2025-06-21 01:37:03,393 - INFO - ============================================================\n",
      "\n",
      "üéâ Collection complete. Check logs and output files for details.\n",
      "‚û°Ô∏è Proceed to the next cell for preliminary analysis.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: DATA COLLECTION ORCHESTRATOR\n",
    "# =============================================================================\n",
    "\n",
    "def run_collection_pipeline():\n",
    "    \"\"\"Orchestrates the entire social media data collection process.\"\"\"\n",
    "    logger.info(\"üöÄ Starting social media data collection pipeline...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # --- Execute collection for each platform ---\n",
    "    twitter_data = collector.fetch_from_twitter()\n",
    "    reddit_data = collector.fetch_from_reddit()\n",
    "    facebook_data = collector.fetch_from_facebook() # Placeholder call\n",
    "    \n",
    "    # --- Summarize Results ---\n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"‚úÖ SOCIAL MEDIA DATA COLLECTION COMPLETED!\")\n",
    "    logger.info(f\"‚è±Ô∏è  Total duration: {duration}\")\n",
    "    logger.info(f\"üìä Records collected:\")\n",
    "    logger.info(f\"   - Twitter: {len(twitter_data)} records saved to {config.OUTPUT_PATHS['twitter']}\")\n",
    "    logger.info(f\"   - Reddit: {len(reddit_data)} records saved to {config.OUTPUT_PATHS['reddit']}\")\n",
    "    logger.info(f\"   - Facebook: {len(facebook_data)} records (placeholder)\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    print(\"\\nüéâ Collection complete. Check logs and output files for details.\")\n",
    "    print(\"‚û°Ô∏è Proceed to the next cell for preliminary analysis.\")\n",
    "\n",
    "# --- Execute the Pipeline ---\n",
    "run_collection_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Loading and Preliminary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 01:37:25,117 - INFO - üîé Loading and analyzing collected data...\n",
      "\n",
      "--- Data Analysis Report ---\n",
      "\n",
      "üìà Twitter Data Overview:\n",
      "   - Total records: 12\n",
      "   - Columns: ['platform', 'id', 'text', 'created_at', 'author_id']\n",
      "\n",
      "   Sample Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>platform</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>author_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1936013144666894505</td>\n",
       "      <td>Saw a lot of very questionably dressed people ...</td>\n",
       "      <td>2025-06-20 10:47:57+00:00</td>\n",
       "      <td>995284287288033284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1935786917443826001</td>\n",
       "      <td>@JodiK46062719 Its melbourne. I doubt its the ...</td>\n",
       "      <td>2025-06-19 19:49:00+00:00</td>\n",
       "      <td>1843659927027372032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1935660223513215226</td>\n",
       "      <td>Last night in the big smoke (hopefully for a l...</td>\n",
       "      <td>2025-06-19 11:25:34+00:00</td>\n",
       "      <td>198704132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  platform                   id  \\\n",
       "0  twitter  1936013144666894505   \n",
       "1  twitter  1935786917443826001   \n",
       "2  twitter  1935660223513215226   \n",
       "\n",
       "                                                text  \\\n",
       "0  Saw a lot of very questionably dressed people ...   \n",
       "1  @JodiK46062719 Its melbourne. I doubt its the ...   \n",
       "2  Last night in the big smoke (hopefully for a l...   \n",
       "\n",
       "                 created_at            author_id  \n",
       "0 2025-06-20 10:47:57+00:00   995284287288033284  \n",
       "1 2025-06-19 19:49:00+00:00  1843659927027372032  \n",
       "2 2025-06-19 11:25:34+00:00            198704132  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü° Reddit data not found or file is empty. Skipped.\n",
      "\n",
      "üü° Facebook data not found or file is empty. Skipped.\n",
      "--- End of Report ---\n",
      "\n",
      "‚úÖ DataFrames are loaded and returned. You can access them via the 'loaded_dfs' variable.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: DATA LOADING AND PRELIMINARY ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_analyze_data():\n",
    "    \"\"\"Loads the collected JSON data into Pandas DataFrames and shows a summary.\"\"\"\n",
    "    logger.info(\"üîé Loading and analyzing collected data...\")\n",
    "    dataframes = {}\n",
    "    \n",
    "    print(\"\\n--- Data Analysis Report ---\")\n",
    "    \n",
    "    for platform, path in config.OUTPUT_PATHS.items():\n",
    "        if os.path.exists(path) and os.path.getsize(path) > 0:\n",
    "            try:\n",
    "                df = pd.read_json(path)\n",
    "                dataframes[platform] = df\n",
    "                print(f\"\\nüìà {platform.title()} Data Overview:\")\n",
    "                print(f\"   - Total records: {len(df):,}\")\n",
    "                print(f\"   - Columns: {list(df.columns)}\")\n",
    "                print(\"\\n   Sample Data:\")\n",
    "                display(df.head(3))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Failed to load and analyze {platform} data from {path}: {e}\")\n",
    "        else:\n",
    "            print(f\"\\nüü° {platform.title()} data not found or file is empty. Skipped.\")\n",
    "            \n",
    "    print(\"--- End of Report ---\")\n",
    "    \n",
    "    if not dataframes:\n",
    "        print(\"\\nNo data was loaded. Please check the collection logs.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n‚úÖ DataFrames are loaded and returned. You can access them via the 'loaded_dfs' variable.\")\n",
    "    return dataframes\n",
    "\n",
    "# --- Run the analysis on the generated files ---\n",
    "# You can re-run this cell anytime after the collection is complete.\n",
    "loaded_dfs = load_and_analyze_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
