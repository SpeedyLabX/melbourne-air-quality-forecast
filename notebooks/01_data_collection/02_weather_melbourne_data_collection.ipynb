{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a731157",
   "metadata": {},
   "source": [
    "# Melbourne Weather Historical Data Collection\n",
    "# Research Notebook for Environmental Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d13201",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook fetches historical weather data for the same locations as the air quality monitoring stations across Melbourne. The primary data source is the Australian Bureau of Meteorology (BOM), typically accessed via pre-compiled CSV files for specific weather stations.\n",
    "\n",
    "### Key Features:\n",
    "- **Aligned Locations**: Uses the same 23 locations as the air quality notebook for easy data merging.\n",
    "- **Simulated Download**: Simulates the process of downloading and processing individual station CSVs.\n",
    "- **Robust Error Handling**: Manages file not found errors and data processing issues.\n",
    "- **Research-Ready Output**: Aggregates data into a single, clean CSV file.\n",
    "- **Professional Logging**: Comprehensive, UTF-8 aware logging for traceability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92644dca",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cacfcc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-20 20:54:27,166 - INFO - üå¶Ô∏è Melbourne Weather Data Collection System\n",
      "2025-06-20 20:54:27,168 - INFO - ==================================================\n",
      "2025-06-20 20:54:27,168 - INFO - üî¨ Research Environment Initialized\n",
      "2025-06-20 20:54:27,170 - INFO - üìã Dependencies loaded and UTF-8 logger configured.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: SETUP, IMPORTS, AND LOGGER CONFIGURATION\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Melbourne Weather Data Collection System\n",
    "========================================\n",
    "\n",
    "This notebook collects historical weather data from the Bureau of Meteorology (BOM)\n",
    "for locations corresponding to EPA Victoria monitoring stations.\n",
    "\n",
    "Author: Research Team\n",
    "Date: 2025\n",
    "Purpose: Environmental analysis and building a multimodal dataset.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. UTF-8 Aware Logger Setup (Identical to previous notebook for consistency) ---\n",
    "def setup_logger(log_file='logs/02_weather_melbourne_data_collection.log', level=logging.INFO):\n",
    "    \"\"\"Configures a logger to be UTF-8 aware for both console and file output.\"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(level)\n",
    "    \n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    # Console Handler\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "    \n",
    "    # File Handler\n",
    "    file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# --- 2. Initialize Environment ---\n",
    "logger = setup_logger()\n",
    "\n",
    "logger.info(\"üå¶Ô∏è Melbourne Weather Data Collection System\")\n",
    "logger.info(\"=\" * 50)\n",
    "logger.info(\"üî¨ Research Environment Initialized\")\n",
    "logger.info(\"üìã Dependencies loaded and UTF-8 logger configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9809e8",
   "metadata": {},
   "source": [
    "## 2. Configuration and Research Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a14a1dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-20 21:05:43,469 - INFO - üéØ Weather Research Configuration Loaded\n",
      "2025-06-20 21:05:43,471 - INFO - üìç Processing 23 locations\n",
      "2025-06-20 21:05:43,472 - INFO - üíæ Output file will be: ../../data/raw/melbourne_raw_weather_20201125_to_20250104.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: RESEARCH CONFIGURATION FOR BOM DATA\n",
    "# =============================================================================\n",
    "\n",
    "class WeatherDataConfig:\n",
    "    \"\"\"Configuration class for BOM weather data collection.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Research Period (Should match the air quality data for consistency)\n",
    "        self.START_DATE = datetime(2020, 11, 25)\n",
    "        self.END_DATE = datetime(2025, 1, 4)\n",
    "        \n",
    "        # Data Source Configuration (Simulating downloaded BOM CSVs)\n",
    "        # In a real-world scenario, you would have a list of URLs to download from.\n",
    "        # Here, we assume they are pre-downloaded and stored locally.\n",
    "        self.BOM_DATA_SOURCE_DIR = \"../data/bom_station_data/\"\n",
    "        \n",
    "        # Output Configuration\n",
    "        self.OUTPUT_CSV_PATH = f\"../../data/raw/melbourne_raw_weather_{self.START_DATE.strftime('%Y%m%d')}_to_{self.END_DATE.strftime('%Y%m%d')}.csv\"\n",
    "        \n",
    "        # Mapping from our research locations to the nearest BOM station ID and its filename\n",
    "        # NOTE: This is a CRITICAL mapping step. You must identify the closest/most representative BOM station for each location.\n",
    "        # The station ID (e.g., '086071') is key.\n",
    "        self.LOCATION_TO_BOM_STATION = {\n",
    "            \"Melbourne CBD\": {\"id\": \"086071\", \"file\": \"BOM_station_086071_Melbourne.csv\"},\n",
    "            \"Footscray\": {\"id\": \"086071\", \"file\": \"BOM_station_086071_Melbourne.csv\"},\n",
    "            \"Brooklyn\": {\"id\": \"087031\", \"file\": \"BOM_station_087031_Laverton.csv\"},\n",
    "            \"Alphington\": {\"id\": \"086071\", \"file\": \"BOM_station_086071_Melbourne.csv\"},\n",
    "            \"Spotswood\": {\"id\": \"087031\", \"file\": \"BOM_station_087031_Laverton.csv\"},\n",
    "            \"Box Hill\": {\"id\": \"086068\", \"file\": \"BOM_station_086068_Scoresby.csv\"},\n",
    "            \"Brighton\": {\"id\": \"086077\", \"file\": \"BOM_station_086077_Moorabbin.csv\"},\n",
    "            \"Dandenong\": {\"id\": \"086068\", \"file\": \"BOM_station_086068_Scoresby.csv\"},\n",
    "            \"Mooroolbark\": {\"id\": \"086068\", \"file\": \"BOM_station_086068_Scoresby.csv\"},\n",
    "            \"Altona North\": {\"id\": \"087031\", \"file\": \"BOM_station_087031_Laverton.csv\"},\n",
    "            \"Melton\": {\"id\": \"087031\", \"file\": \"BOM_station_087031_Laverton.csv\"}, # Assuming Laverton is closest\n",
    "            \"Point Cook\": {\"id\": \"087031\", \"file\": \"BOM_station_087031_Laverton.csv\"},\n",
    "            \"Macleod\": {\"id\": \"086071\", \"file\": \"BOM_station_086071_Melbourne.csv\"},\n",
    "            \"Carlton\": {\"id\": \"086071\", \"file\": \"BOM_station_086071_Melbourne.csv\"},\n",
    "            \"Richmond\": {\"id\": \"086071\", \"file\": \"BOM_station_086071_Melbourne.csv\"},\n",
    "            \"St Kilda\": {\"id\": \"086071\", \"file\": \"BOM_station_086071_Melbourne.csv\"},\n",
    "            \"Yarraville\": {\"id\": \"086071\", \"file\": \"BOM_station_086071_Melbourne.csv\"},\n",
    "            \"Frankston\": {\"id\": \"086077\", \"file\": \"BOM_station_086077_Moorabbin.csv\"}, # Assuming Moorabbin\n",
    "            \"Ringwood\": {\"id\": \"086068\", \"file\": \"BOM_station_086068_Scoresby.csv\"},\n",
    "            \"Werribee\": {\"id\": \"087031\", \"file\": \"BOM_station_087031_Laverton.csv\"},\n",
    "            \"Craigieburn\": {\"id\": \"086038\", \"file\": \"BOM_station_086038_Essendon.csv\"},\n",
    "            \"Pakenham\": {\"id\": \"086068\", \"file\": \"BOM_station_086068_Scoresby.csv\"},\n",
    "            \"Broadmeadows\": {\"id\": \"086038\", \"file\": \"BOM_station_086038_Essendon.csv\"},\n",
    "        }\n",
    "        \n",
    "        # These are the headers for our FINAL aggregated CSV file\n",
    "        self.CSV_HEADERS = [\n",
    "            \"location\", \"date\", \"t_max\", \"t_min\", \"precip\", \"humidity_9am\", \"wind_speed_9am\"\n",
    "        ]\n",
    "\n",
    "# --- Initialize configuration ---\n",
    "try:\n",
    "    config = WeatherDataConfig()\n",
    "    logger.info(f\"üéØ Weather Research Configuration Loaded\")\n",
    "    logger.info(f\"üìç Processing {len(config.LOCATION_TO_BOM_STATION)} locations\")\n",
    "    logger.info(f\"üíæ Output file will be: {config.OUTPUT_CSV_PATH}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not initialize configuration: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cbb87",
   "metadata": {},
   "source": [
    "## 3. Data Collection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94d72033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-20 21:05:48,027 - INFO - üîß Weather data collector class defined and instance created.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: CORE DATA COLLECTION CLASS FOR BOM DATA\n",
    "# =============================================================================\n",
    "\n",
    "class WeatherDataCollector:\n",
    "    \"\"\"Handles the mechanics of reading, processing, and aggregating BOM CSVs.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: WeatherDataConfig):\n",
    "        self.config = config\n",
    "        self.total_records = 0\n",
    "        self.processed_files = set() # To avoid reading the same file multiple times\n",
    "        self.station_dataframes = {} # Cache for loaded data\n",
    "\n",
    "    def _read_and_cache_station_data(self, station_file: str) -> bool:\n",
    "        \"\"\"Reads a BOM station CSV file and stores it in a cache.\"\"\"\n",
    "        if station_file in self.processed_files:\n",
    "            return True # Already loaded\n",
    "            \n",
    "        full_path = os.path.join(self.config.BOM_DATA_SOURCE_DIR, station_file)\n",
    "        try:\n",
    "            # BOM CSVs often have header rows to skip\n",
    "            df = pd.read_csv(full_path, skiprows=8)\n",
    "            # Standardize column names\n",
    "            df.rename(columns={\n",
    "                'Date': 'date',\n",
    "                'Maximum temperature (Degree C)': 't_max',\n",
    "                'Minimum temperature (Degree C)': 't_min',\n",
    "                'Rainfall (mm)': 'precip',\n",
    "                '9am relative humidity (%)': 'humidity_9am',\n",
    "                '9am wind speed (km/h)': 'wind_speed_9am'\n",
    "            }, inplace=True)\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            self.station_dataframes[station_file] = df\n",
    "            self.processed_files.add(station_file)\n",
    "            logger.info(f\"‚úÖ Successfully loaded and cached data from '{station_file}'.\")\n",
    "            return True\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"‚ùå File not found: {full_path}. This location will be skipped.\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to process file {full_path}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _get_data_for_location(self, location_name: str, station_file: str) -> List[Dict]:\n",
    "        \"\"\"Extracts data for a specific location from the cached DataFrame.\"\"\"\n",
    "        if station_file not in self.station_dataframes:\n",
    "            return []\n",
    "        \n",
    "        df = self.station_dataframes[station_file]\n",
    "        \n",
    "        # Filter by the research date range\n",
    "        mask = (df['date'] >= self.config.START_DATE) & (df['date'] <= self.config.END_DATE)\n",
    "        df_filtered = df.loc[mask].copy()\n",
    "        \n",
    "        # Add the research location name to each record\n",
    "        df_filtered['location'] = location_name\n",
    "        \n",
    "        # Convert to a list of dictionaries for writing\n",
    "        return df_filtered[self.config.CSV_HEADERS].to_dict('records')\n",
    "\n",
    "    def collect_all_data(self) -> Tuple[bool, int]:\n",
    "        \"\"\"Orchestrates the entire data aggregation process.\"\"\"\n",
    "        logger.info(\"üöÄ Starting comprehensive weather data aggregation...\")\n",
    "        self.total_records = 0\n",
    "        \n",
    "        # Step 1: Pre-load all necessary BOM station files\n",
    "        unique_station_files = {v['file'] for v in self.config.LOCATION_TO_BOM_STATION.values()}\n",
    "        for station_file in tqdm(unique_station_files, desc=\"üìÇ Caching Station Files\"):\n",
    "            self._read_and_cache_station_data(station_file)\n",
    "            \n",
    "        # Step 2: Write data for each location to the final CSV\n",
    "        try:\n",
    "            with open(self.config.OUTPUT_CSV_PATH, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "                writer = csv.DictWriter(file, fieldnames=self.config.CSV_HEADERS)\n",
    "                writer.writeheader()\n",
    "                location_progress = tqdm(self.config.LOCATION_TO_BOM_STATION.items(), desc=\"üåç Processing Locations\", unit=\"location\")\n",
    "                \n",
    "                for location, station_info in location_progress:\n",
    "                    location_progress.set_postfix_str(f\"üìç {location}\")\n",
    "                    records = self._get_data_for_location(location, station_info['file'])\n",
    "                    if records:\n",
    "                        writer.writerows(records)\n",
    "                        self.total_records += len(records)\n",
    "            return True, self.total_records\n",
    "        except IOError as e:\n",
    "            logger.critical(f\"üí• CRITICAL FILE ERROR: Could not write to {self.config.OUTPUT_CSV_PATH}. Reason: {e}\")\n",
    "            return False, 0\n",
    "\n",
    "# --- Initialize the collector ---\n",
    "collector = WeatherDataCollector(config)\n",
    "logger.info(\"üîß Weather data collector class defined and instance created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a350132",
   "metadata": {},
   "source": [
    "## 4. Execute Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f192c30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-20 21:05:53,542 - INFO - üïê Collection started at: 2025-06-20 21:05:53\n",
      "2025-06-20 21:05:53,543 - INFO - üöÄ Starting comprehensive weather data aggregation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec03da59d664a68a908eef7094980cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üìÇ Caching Station Files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-20 21:05:53,577 - ERROR - ‚ùå File not found: ../data/bom_station_data/BOM_station_086068_Scoresby.csv. This location will be skipped.\n",
      "2025-06-20 21:05:53,579 - ERROR - ‚ùå File not found: ../data/bom_station_data/BOM_station_087031_Laverton.csv. This location will be skipped.\n",
      "2025-06-20 21:05:53,580 - ERROR - ‚ùå File not found: ../data/bom_station_data/BOM_station_086071_Melbourne.csv. This location will be skipped.\n",
      "2025-06-20 21:05:53,582 - ERROR - ‚ùå File not found: ../data/bom_station_data/BOM_station_086077_Moorabbin.csv. This location will be skipped.\n",
      "2025-06-20 21:05:53,583 - ERROR - ‚ùå File not found: ../data/bom_station_data/BOM_station_086038_Essendon.csv. This location will be skipped.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af20f00b280541ab9906c26fe957d743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üåç Processing Locations:   0%|          | 0/23 [00:00<?, ?location/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-20 21:05:53,602 - INFO - ============================================================\n",
      "2025-06-20 21:05:53,606 - INFO - ‚úÖ WEATHER DATA AGGREGATION COMPLETED SUCCESSFULLY!\n",
      "2025-06-20 21:05:53,607 - INFO - üìÅ Data saved to: ../../data/raw/melbourne_raw_weather_20201125_to_20250104.csv\n",
      "2025-06-20 21:05:53,608 - INFO - üìä Total records aggregated: 0\n",
      "2025-06-20 21:05:53,609 - INFO - ‚è±Ô∏è  Total duration: 0:00:00.059187\n",
      "\n",
      "üéâ Ready for next stage of processing! See logs for details.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: DATA COLLECTION ORCHESTRATOR (Identical structure to previous notebook)\n",
    "# =============================================================================\n",
    "\n",
    "def _print_results_summary(start_time: datetime, end_time: datetime, total_records: int):\n",
    "    \"\"\"Logs the summary of the data collection results.\"\"\"\n",
    "    duration = end_time - start_time\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"‚úÖ WEATHER DATA AGGREGATION COMPLETED SUCCESSFULLY!\")\n",
    "    logger.info(f\"üìÅ Data saved to: {config.OUTPUT_CSV_PATH}\")\n",
    "    logger.info(f\"üìä Total records aggregated: {total_records}\")\n",
    "    logger.info(f\"‚è±Ô∏è  Total duration: {duration}\")\n",
    "\n",
    "def _print_final_guidance(success: bool):\n",
    "    \"\"\"Prints helpful next steps or troubleshooting advice to the console.\"\"\"\n",
    "    if success:\n",
    "        print(\"\\nüéâ Ready for next stage of processing! See logs for details.\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå DATA COLLECTION FAILED. Check the log file for detailed error information.\")\n",
    "\n",
    "def run_data_collection_pipeline() -> bool:\n",
    "    \"\"\"Orchestrates the entire data collection process.\"\"\"\n",
    "    logger.info(f\"üïê Collection started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    success, total_records = collector.collect_all_data()\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    if success:\n",
    "        _print_results_summary(start_time, end_time, total_records)\n",
    "    \n",
    "    return success\n",
    "\n",
    "# --- Execute the Pipeline ---\n",
    "was_successful = run_data_collection_pipeline()\n",
    "_print_final_guidance(was_successful)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f270c6b",
   "metadata": {},
   "source": [
    "## 5. Data Analysis and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd511b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-20 21:07:54,042 - INFO - üìä Analyzing collected data from '../../data/raw/melbourne_raw_weather_20201125_to_20250104.csv'...\n",
      "\n",
      "--- Data Analysis Report ---\n",
      "üìà Dataset Overview:\n",
      "   - Total records: 0\n",
      "   - Unique Locations: 0\n",
      "\n",
      "üìç Location Coverage (Top 10):\n",
      "Series([], Name: location, dtype: int64)\n",
      "\n",
      "üî¨ Data Completeness per Parameter:\n",
      "   - t_max: nan% complete\n",
      "   - t_min: nan% complete\n",
      "   - precip: nan% complete\n",
      "   - humidity_9am: nan% complete\n",
      "   - wind_speed_9am: nan% complete\n",
      "\n",
      "‚úÖ Data Quality Checks:\n",
      "   - Duplicate rows: 0\n",
      "   - Rows with missing dates: 0\n",
      "--- End of Report ---\n",
      "\n",
      "DataFrame returned to 'df_analysis' variable. You can now use it for further work.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: DATA ANALYSIS AND VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_collected_data():\n",
    "    \"\"\"Perform initial analysis and validation of the collected data CSV.\"\"\"\n",
    "    if not os.path.exists(config.OUTPUT_CSV_PATH):\n",
    "        logger.error(f\"‚ùå No data file found at '{config.OUTPUT_CSV_PATH}'. Please run data collection first.\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(f\"üìä Analyzing collected data from '{config.OUTPUT_CSV_PATH}'...\")\n",
    "    try:\n",
    "        df = pd.read_csv(config.OUTPUT_CSV_PATH)\n",
    "        \n",
    "        print(\"\\n--- Data Analysis Report ---\")\n",
    "        print(f\"üìà Dataset Overview:\")\n",
    "        print(f\"   - Total records: {len(df):,}\")\n",
    "        print(f\"   - Unique Locations: {df['location'].nunique()}\")\n",
    "        \n",
    "        print(f\"\\nüìç Location Coverage (Top 10):\")\n",
    "        print(df['location'].value_counts().head(10))\n",
    "        \n",
    "        print(f\"\\nüî¨ Data Completeness per Parameter:\")\n",
    "        weather_params = ['t_max', 't_min', 'precip', 'humidity_9am', 'wind_speed_9am']\n",
    "        for param in weather_params:\n",
    "            if param in df.columns:\n",
    "                percentage = df[param].notna().mean() * 100\n",
    "                print(f\"   - {param}: {percentage:.1f}% complete\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Data Quality Checks:\")\n",
    "        print(f\"   - Duplicate rows: {df.duplicated().sum()}\")\n",
    "        print(f\"   - Rows with missing dates: {df['date'].isnull().sum()}\")\n",
    "        print(\"--- End of Report ---\")\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Analysis failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Run the analysis on the generated file ---\n",
    "df_analysis = analyze_collected_data()\n",
    "if df_analysis is not None:\n",
    "    print(\"\\nDataFrame returned to 'df_analysis' variable. You can now use it for further work.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
